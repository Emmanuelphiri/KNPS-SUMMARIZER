# -*- coding: utf-8 -*-
"""summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16HN_LJJ6Ir_rFBssenQorWKs59K_TjPv

# Colab output wrapper
"""

# wrap the output in colab cells
from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

"""# Install Transformers"""

# install transformers with sentencepiece
!pip install transformers[sentencepiece]

"""# Read input file from Google Drive"""

FileContent = "Yeah. Yeah, sure. It kinda does make sense, doesn't it, because when we get into the end of meeting we're kind of talking about action and design as opposed to background. Everything I have is kinda background. Mm-hmm. Uh that sounds. Sure. Okay. Sure. Yeah, cool. Why don't I get that? Hmm. Okay. Okay. Um alright so c is it function F_ eight? Hmm. Come on. I think it's working. Okay great s so let me just start this. Okay great. So um uh s move on. Uh-huh oh where'd it all go? It's not good. Okay lemme just see where I can find it. This looks more like it. I think I just opened up the template. Sorry about that. Okay alright so let's have a look here. Okay so this was the method that um I've taken. Uh basically what I wanna do here, before we get into it uh too far, is I want to show you all the background information I have that I think we need to acknowledge if we want this to be successful. And uh and then sorta g go through some of the way that I've dealt with that information, and"

# display file content
FileContent

# total characters in the file
len(FileContent)

"""# Load the Model and Tokenizer"""

# import and initialize the tokenizer and model from the checkpoint
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

checkpoint = "sshleifer/distilbart-cnn-12-6"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

"""# Some model statistics"""

# max tokens including the special tokens
tokenizer.model_max_length

# max tokens excluding the special tokens
tokenizer.max_len_single_sentence

# number of special tokens
tokenizer.num_special_tokens_to_add()

"""# Convert file content to sentences"""

# extract the sentences from the document
import nltk
nltk.download('punkt')
sentences = nltk.tokenize.sent_tokenize(FileContent)

# find the max tokens in the longest sentence
max([len(tokenizer.tokenize(sentence)) for sentence in sentences])

"""# Create the chunks"""

# initialize
length = 0
chunk = ""
chunks = []
count = -1
for sentence in sentences:
  count += 1
  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter

  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed
    chunk += sentence + " " # add the sentence to the chunk
    length = combined_length # update the length counter

    # if it is the last sentence
    if count == len(sentences) - 1:
      chunks.append(chunk.strip()) # save the chunk

  else:
    chunks.append(chunk.strip()) # save the chunk

    # reset
    length = 0
    chunk = ""

    # take care of the overflow sentence
    chunk += sentence + " "
    length = len(tokenizer.tokenize(sentence))
len(chunks)

"""# Some checks"""

[len(tokenizer.tokenize(c)) for c in chunks]

[len(tokenizer(c).input_ids) for c in chunks]

"""## With special tokens added"""

sum([len(tokenizer(c).input_ids) for c in chunks])

len(tokenizer(FileContent).input_ids)

"""## Without special tokens added"""

sum([len(tokenizer.tokenize(c)) for c in chunks])

len(tokenizer.tokenize(FileContent))

"""# Get the inputs"""

# inputs to the model
inputs = [tokenizer(chunk, return_tensors="pt") for chunk in chunks]

"""# Output"""

for input in inputs:
  output = model.generate(**input)
  print(tokenizer.decode(*output, skip_special_tokens=True))